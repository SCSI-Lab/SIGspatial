{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #  bayesian-optimization 라이브러리의 BayesianOptimization 클래스 import\n",
    "  from bayes_opt import BayesianOptimization\n",
    "  import numpy as np\n",
    "\n",
    "  # 실험해보고자하는 hyperparameter 집합\n",
    "  pbounds = {'max_depth': (3, 7),\n",
    "                'learning_rate': (0.01, 0.2),\n",
    "                'n_estimators': (5000, 10000),\n",
    "                'gamma': (0, 100),\n",
    "                'min_child_weight': (0, 3),\n",
    "                'subsample': (0.5, 1),\n",
    "                'colsample_bytree' :(0.2, 1)\n",
    "                }\n",
    "\n",
    "  # Bayesian optimization 객체 생성\n",
    "  # f : 탐색 대상 함수, pbounds : hyperparameter 집합\n",
    "  # verbose = 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
    "  # random_state : Bayesian Optimization 상의 랜덤성이 존재하는 부분을 통제 \n",
    "  bo=BayesianOptimization(f=XGB_cv, pbounds=pbounds, verbose=2, random_state=1 )    \n",
    "\n",
    "  # 메소드를 이용해 최대화 과정 수행\n",
    "  # init_points :  초기 Random Search 갯수\n",
    "  # n_iter : 반복 횟수 (몇개의 입력값-함숫값 점들을 확인할지! 많을 수록 정확한 값을 얻을 수 있다.)\n",
    "  # acq : Acquisition Function들 중 Expected Improvement(EI) 를 사용\n",
    "  # xi : exploration 강도 (기본값은 0.0)\n",
    "  bo.maximize(init_points=2, n_iter=10, acq='ei', xi=0.01)\n",
    "\n",
    "  # ‘iter’는 반복 회차, ‘target’은 목적 함수의 값, 나머지는 입력값을 나타냅니다. \n",
    "  # 현재 회차 이전까지 조사된 함숫값들과 비교하여, 현재 회차에 최댓값이 얻어진 경우, \n",
    "  # bayesian-optimization 라이브러리는 이를 자동으로 다른 색 글자로 표시하는 것을 확인할 수 있습니다\n",
    "\n",
    "  # 찾은 파라미터 값 확인\n",
    "  print(bo.max)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
