{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is a Post-Processing part. Just Follow Our Step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "from shapely import Polygon\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduction\n",
    "\n",
    "1. elipsoid_calculate(),  this func used to calculate polygon's width and height ratio.\n",
    "2. select_polygons_based_on_condition(), this func used to eliminate polygon which unsuitable for area & width, height ratio condition.\n",
    "3. extract_mean_ndwi() + mean_thresholding(), this func used to eliminate the lakes which lakes NDWI band's mean value under our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elipsoid_calculate(polygon):\n",
    "    exterior_ring = polygon.exterior\n",
    "    envelope = exterior_ring.envelope\n",
    "    envelope_coords = list(envelope.exterior.coords)\n",
    "\n",
    "    r_x = (envelope_coords[2][0] - envelope_coords[0][0]) / 2\n",
    "    r_y = (envelope_coords[2][1] - envelope_coords[0][1]) / 2\n",
    "\n",
    "    if r_x/r_y > 10 or r_y/r_x > 10:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_ndwi(bbox, geotiff_data, geotiff_transform):\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    col_start, row_start = ~geotiff_transform * (minx, maxy)\n",
    "    col_end, row_end = ~geotiff_transform * (maxx, miny)\n",
    "    row_start, row_end = int(row_start), int(row_end)\n",
    "    col_start, col_end = int(col_start), int(col_end)\n",
    "    clipped_data = geotiff_data[row_start:row_end, col_start:col_end]\n",
    "    return np.mean(clipped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_polygons_based_on_condition(input_path, threshold=100000.0, buf_dist=30):\n",
    "\n",
    "    threshold = 100000.0   # 0.1 km^2 이상 면적\n",
    "    buf_dist = 30 # 한 칸의 대각선의 길이가 대략 20m인 점을 고려\n",
    "\n",
    "    poly_arr = []\n",
    "\n",
    "    gdf = gpd.read_file(input_path)\n",
    "    gdf_buffered = gdf.explode(index_parts=True).buffer(buf_dist)\n",
    "    gdf_buffered = gpd.GeoDataFrame(geometry=gdf_buffered)\n",
    "    combined_polygons = gdf_buffered.dissolve().explode(index_parts=True).buffer(-buf_dist)\n",
    "\n",
    "    for poly in combined_polygons:\n",
    "\n",
    "        if poly.geom_type == 'Polygon':\n",
    "            polygon = poly\n",
    "            if elipsoid_calculate(polygon) and polygon.area >= threshold:\n",
    "                poly_arr.append(polygon)\n",
    "\n",
    "        elif poly.geom_type == 'MultiPolygon':\n",
    "            for polygon in poly.geoms:\n",
    "                if elipsoid_calculate(polygon) and polygon.area >= threshold:\n",
    "                    poly_arr.append(polygon)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(geometry=poly_arr)\n",
    "    gdf.crs = \"EPSG:3857\"\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_thresholding(Image_path, gdf):\n",
    "    with rasterio.open(Image_path,'r+') as src:\n",
    "        geotiff_profile = src.profile\n",
    "        geotiff_data = src.read(6)  # Assuming 1-band GeoTIFF\n",
    "        src.crs = gdf.crs\n",
    "\n",
    "    Image_name = Image_path.split(\"/\")[-1]\n",
    "\n",
    "    filtered_multipolygons = []\n",
    "    \n",
    "    for idx, row in gdf.iterrows():\n",
    "        bbox = row['geometry'].bounds\n",
    "        mean_ndwi = extract_mean_ndwi(bbox, geotiff_data, geotiff_profile[\"transform\"])\n",
    "        #print(mean_ndwi)\n",
    "        \n",
    "        if mean_ndwi > 0.3:\n",
    "            filtered_multipolygons.append(row)\n",
    "\n",
    "    # 필터링된 MultiPolygon으로 GeoDataFrame 생성\n",
    "    filtered_gdf = gpd.GeoDataFrame(filtered_multipolygons, crs=gdf.crs, geometry='geometry')\n",
    "    filtered_gdf['Image'] = Image_name\n",
    "    # Step 6: 필터링된 MultiPolygon을 저장\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read region shape file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file exist in our github. you can download it as our github repository folder named \"region_shape_file\"  \n",
    "After you download the shpaefile, add on region_file pathes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region1_file =\"C:/Users/scsi/Desktop/Jiwan/SIG/forGIT/SIGspatial/region_shape_file/region_num_1.shp\"\n",
    "region3_file =\"C:/Users/scsi/Desktop/Jiwan/SIG/forGIT/SIGspatial/region_shape_file/region_num_3.shp\"\n",
    "region5_file =\"C:/Users/scsi/Desktop/Jiwan/SIG/forGIT/SIGspatial/region_shape_file/region_num_5.shp\"\n",
    "\n",
    "region2_file =\"C:/Users/scsi/Desktop/Jiwan/SIG/forGIT/SIGspatial/region_shape_file/region_num_2.shp\"\n",
    "region4_file =\"C:/Users/scsi/Desktop/Jiwan/SIG/forGIT/SIGspatial/region_shape_file/region_num_4.shp\"\n",
    "region6_file =\"C:/Users/scsi/Desktop/Jiwan/SIG/forGIT/SIGspatial/region_shape_file/region_num_6.shp\"\n",
    "\n",
    "region1 = gpd.read_file(region1_file)\n",
    "region3 = gpd.read_file(region3_file)\n",
    "region5 = gpd.read_file(region5_file)\n",
    "\n",
    "region2 = gpd.read_file(region2_file)\n",
    "region4 = gpd.read_file(region4_file)\n",
    "region6 = gpd.read_file(region6_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add_region(), this function use to add attribute_table column named 'region'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_region(gdf, save_path):\n",
    "    \n",
    "    intersection1 = gpd.overlay(region1, gdf, how = 'intersection' )\n",
    "    intersection3 = gpd.overlay(region3, gdf, how = 'intersection' )\n",
    "    intersection5 = gpd.overlay(region5, gdf, how = 'intersection' )\n",
    "\n",
    "    intersection2 = gpd.overlay(region2, gdf, how = 'intersection' )\n",
    "    intersection4 = gpd.overlay(region4, gdf, how = 'intersection' )\n",
    "    intersection6 = gpd.overlay(region6, gdf, how = 'intersection' )\n",
    "\n",
    "    intersection1 = intersection1.drop(columns=['fid'])\n",
    "    intersection3 = intersection3.drop(columns=['fid'])\n",
    "    intersection5 = intersection5.drop(columns=['fid'])\n",
    "\n",
    "    intersection2 = intersection2.drop(columns=['fid'])\n",
    "    intersection4 = intersection4.drop(columns=['fid'])\n",
    "    intersection6 = intersection6.drop(columns=['fid'])\n",
    "\n",
    "    merged_intersection = gpd.GeoDataFrame(pd.concat([intersection1, intersection3, intersection5,intersection2, intersection4, intersection6], ignore_index=True))\n",
    "    merged_intersection.to_file(save_path, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the pathdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " 1. Image_path, You can download original .tif file in Google drive.  \n",
    " 2. Input_path, If you learn our model, your result save in that path.  \n",
    " 3. Save_path, This path is a save root, After our post-processing in each .gpkg file.dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_path = \"D:/SIG/9.final_data/Greenland_sentinel2_19-06-03_region_5.tif\" # Original Image path, careful for date-region.\n",
    "input_path = \"C:/Users/scsi/Desktop/result/0603_region5.gpkg\" #your model result gpkg path, also careful for date-region.\n",
    "save_path = \"C:/Users/scsi/Desktop/result/final_gpkg/0603_region5_final.gpkg\" # each gpkg saved path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all your path setting is correct, this code will generate each .gpkf file in your 'save_path' folder.  \n",
    "Unfortunately, you can edit the all path iteratively, in each implementation.  \n",
    "After you implemented the under cell, please edit your path!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = select_polygons_based_on_condition(input_path)\n",
    "gdf = mean_thresholding(Image_path , gdf)\n",
    "add_region(gdf, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Your 'save_path' are filled with each .gpkg files.  \n",
    "Under cell is a merged all your .gpkg file.  \n",
    "Input the final path when you saved your merged .gpkg file in 'output_gpkg'.  \n",
    "Also, Input your folder path which include each .gpkg files, ( = save_path folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gpkg = \"C:/Users/scsi/Desktop/result/final_merged/lake_final.gpkg\"\n",
    "folder_path = \"C:/Users/scsi/Desktop/result/final_gpkg/\"\n",
    "\n",
    "all_gpkg_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gpkg\"):\n",
    "            gpkg_path = os.path.join(root, file)\n",
    "            all_gpkg_files.append(gpkg_path)\n",
    "\n",
    "all_gdfs = []\n",
    "\n",
    "for gpkg_file in all_gpkg_files:\n",
    "    gdf = gpd.read_file(gpkg_file)\n",
    "    all_gdfs.append(gdf)\n",
    "\n",
    "combined_gdf = gpd.GeoDataFrame(pd.concat(all_gdfs, ignore_index=True))\n",
    "\n",
    "combined_gdf.to_file(output_gpkg, driver='GPKG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
