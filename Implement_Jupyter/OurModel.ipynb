{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 21:01:12.180867: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-01 21:01:12.214089: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7630] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-01 21:01:12.214122: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-01 21:01:12.214151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-01 21:01:12.221400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/u2018144071/.conda/envs/SIG/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/u2018144071/.conda/envs/SIG/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 21:01:12.866844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from keras_unet_collection import models, utils\n",
    "from PIL import Image\n",
    "from shapely.affinity import affine_transform\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import *\n",
    "\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import shutil\n",
    "import sys\n",
    "import tifffile as tiff\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent Visualization Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 20:52:41.743913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /device:GPU:0 with 2 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:17:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# 재귀 깊이 제한\n",
    "sys.setrecursionlimit(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ourModel : model_name, rate, input_channels, delete_channel, hyperparams를 입력으로 받음\n",
    "# 고정 : train/test/valid set\n",
    "# 변수 : model_name, rate, input_channels, delete_channel, hyperparams\n",
    "\n",
    "class ourModel():\n",
    "    def __init__(self, case_name, model_name, input_data_path):\n",
    "        # Constructor to initialize the model\n",
    "        # Loads input data from a JSON file\n",
    "        self.case_name = case_name\n",
    "        self.model_name = model_name\n",
    "        self.base_path = '/home/u2018144071/SIG/result'\n",
    "        \n",
    "        # Loads input data from a JSON file\n",
    "        with open(input_data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        self.input_names = np.array(sorted(data[\"input_names\"]))\n",
    "        self.label_names = np.array(sorted(data[\"label_names\"]))\n",
    "        self.train_input_names = np.array(sorted(data[\"train_input_names\"]))\n",
    "        self.train_label_names = np.array(sorted(data[\"train_label_names\"]))\n",
    "        self.valid_input_names = np.array(sorted(data[\"valid_input_names\"]))\n",
    "        self.valid_label_names = np.array(sorted(data[\"valid_label_names\"]))\n",
    "        self.test_input_names = np.array(sorted(data[\"test_input_names\"]))\n",
    "        self.test_label_names = np.array(sorted(data[\"test_label_names\"]))\n",
    "\n",
    "        print(\"Number of images with supraglacial lake : {0}\".format(len(self.input_names)))\n",
    "        print(\"Training:validation:testing = {}:{}:{}\".format(len(self.train_label_names), len(self.valid_label_names), len(self.test_label_names)))   \n",
    "\n",
    "    def data_pre_processing(self, size, img_channels, input_channels):\n",
    "        # Pre-processes the data for training\n",
    "        self.delete_channel = [x for x in [i for i in range(img_channels)] if x not in input_channels]\n",
    "        self.input_channels = len(input_channels)\n",
    "        self.img_channels = img_channels\n",
    "        self.size = size\n",
    "\n",
    "        self.valid_input = input_data_process(image_to_array(self.valid_input_names, self.size, self.img_channels), self.delete_channel)\n",
    "        self.valid_label = target_data_process(image_to_array(self.valid_label_names, self.size, channel=1))\n",
    "    \n",
    "        self.test_input = input_data_process(image_to_array(self.test_input_names, self.size, self.img_channels), self.delete_channel)\n",
    "        self.test_label = target_data_process(image_to_array(self.test_label_names, self.size, channel=1))\n",
    "    \n",
    "    # 해당 case에 대한 model_training\n",
    "    # hyperparameter tuning 가능!\n",
    "\n",
    "    def model_training(self, params_path):\n",
    "        # Trains the model based on the provided hyperparameters\n",
    "        with open(params_path, 'r') as file:\n",
    "            params = json.load(file)\n",
    "        \n",
    "        # params로부터 model의 hyperparameter를 받음\n",
    "        filter_num_ = params[\"filter_num\"]\n",
    "        activation_ = params[\"activation\"]\n",
    "        atten_activation_ = params[\"atten_activation\"]\n",
    "        attention_ = params[\"attention\"]\n",
    "        output_activation_ = params[\"output_activation\"]\n",
    "        batch_norm_ = params[\"batch_norm\"]\n",
    "        pool_ = params[\"pool\"]\n",
    "        unpool_ = params[\"unpool\"]\n",
    "        backbone_ = params[\"backbone\"]\n",
    "        weights_ = params[\"weights\"]\n",
    "        freeze_backbone_ = params[\"freeze_backbone\"]\n",
    "        freeze_batch_norm_ = params[\"freeze_batch_norm\"]\n",
    "        optimizer_ = params[\"optimizer\"]\n",
    "        learning_rate_ = params[\"learning_rate\"]\n",
    "        N_epoch = params[\"N_epoch\"]\n",
    "        N_batch = params[\"N_batch\"]\n",
    "        N_sample = params[\"N_sample\"]\n",
    "        max_tol = params[\"max_tol\"]\n",
    "        min_del = params[\"min_del\"]\n",
    "\n",
    "        self.params = params\n",
    "        \n",
    "        model = models.att_unet_2d((256, 256, self.input_channels), filter_num=filter_num_, n_labels=2, \n",
    "                                stack_num_down=2, stack_num_up=2, activation=activation_, \n",
    "                                atten_activation=atten_activation_, attention=attention_, output_activation=output_activation_, \n",
    "                                batch_norm=batch_norm_, pool=pool_, unpool=unpool_, \n",
    "                                backbone=backbone_, weights=weights_, \n",
    "                                freeze_backbone=freeze_backbone_, freeze_batch_norm=freeze_batch_norm_, \n",
    "                                name='attunet')\n",
    "        \n",
    "        if optimizer_ == \"SGD\":\n",
    "            optimizer_ = keras.optimizers.SGD(learning_rate=learning_rate_)\n",
    "            \n",
    "        elif optimizer_ == \"Adam\":\n",
    "            optimizer_ = keras.optimizers.Adam(learning_rate=learning_rate_)\n",
    "            \n",
    "        model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer_)\n",
    "        \n",
    "        L_train = int(0.6*len(self.input_names))\n",
    "\n",
    "        tol = 0\n",
    "        # loop over epoches\n",
    "        for epoch in range(N_epoch):\n",
    "            # initial loss record\n",
    "            if epoch == 0:\n",
    "                y_pred = model.predict([self.valid_input])\n",
    "                record = np.mean(keras.losses.categorical_crossentropy(self.valid_label, y_pred))\n",
    "                print('\\tInitial loss = {}'.format(record))\n",
    "            \n",
    "            # loop over batches\n",
    "            for step in range(N_batch): \n",
    "                # selecting smaples for the current batch\n",
    "                ind_train_shuffle = utils.shuffle_ind(L_train)[:N_sample] \n",
    "                \n",
    "                # batch data formation\n",
    "                ## augmentation is not applied\n",
    "                train_input = input_data_process(image_to_array(self.train_input_names[ind_train_shuffle], self.size, self.img_channels), self.delete_channel)\n",
    "                train_label = target_data_process(image_to_array(self.train_label_names[ind_train_shuffle], self.size, channel=1))\n",
    "                \n",
    "                # train on batch\n",
    "                loss_ = model.train_on_batch([train_input,], [train_label,])\n",
    "                # ** training loss is not stored ** #\n",
    "                \n",
    "            # epoch-end validation\n",
    "            y_pred = model.predict([self.valid_input])\n",
    "            record_temp = np.mean(keras.losses.categorical_crossentropy(self.valid_label, y_pred))\n",
    "            # ** validation loss is not stored ** #\n",
    "            \n",
    "            # if loss is reduced\n",
    "            if record - record_temp > min_del:\n",
    "                print('Validation performance is improved from {} to {}'.format(record, record_temp))\n",
    "                record = record_temp; # update the loss record\n",
    "                tol = 0; # refresh early stopping patience \n",
    "                # ** model checkpoint is not stored ** #\n",
    "                \n",
    "            # if loss not reduced\n",
    "            else:\n",
    "                print('Validation performance {} is NOT improved'.format(record_temp))\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping with only {0} epochs.'.format(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    # Pass to the next epoch\n",
    "                    continue\n",
    "                    \n",
    "        print(\"학습 종료\")\n",
    "            \n",
    "        self.model = model\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Saves the trained model\n",
    "        # hyperparameter, model name, threshold, f1_score will save to json file.\n",
    "        model_save_path = os.path.join(self.base_path, self.case_name, self.model_name)\n",
    "        \n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        self.model.save(os.path.join(model_save_path, self.model_name) + \".h5\")\n",
    "\n",
    "        print(\"모델 저장 완료. save path : {0}\".format(model_save_path))\n",
    "\n",
    "    def load_model(self, load_path):\n",
    "        self.model = keras.models.load_model(load_path)\n",
    "\n",
    "    def save_result(self):\n",
    "        # Calculates evaluation metrics and saves results\n",
    "        self.y_pred = self.model.predict([self.test_input])\n",
    "    \n",
    "        cross_entropy = np.mean(keras.losses.categorical_crossentropy(self.test_label, self.y_pred))\n",
    "        threshold = find_best_threshold(self.test_label, self.y_pred)\n",
    "        f1, best_img, worst_img = f1_score(self.test_label, self.y_pred, threshold)\n",
    "        \n",
    "        # save results\n",
    "        self.result_dict = {}\n",
    "        self.result_dict[\"cross_entropy\"] = float(cross_entropy)\n",
    "        self.result_dict[\"f1_score\"] = float(f1)\n",
    "        self.result_dict[\"threshold\"] = float(threshold)\n",
    "        self.result_dict[\"best_img\"] = list(map(int, best_img))\n",
    "        self.result_dict[\"worst_img\"] = list(map(int, worst_img))\n",
    "\n",
    "    def save_only_test_set(self):\n",
    "        # Saves results for the test set\n",
    "        threshold = self.result_dict[\"threshold\"]\n",
    "        y_result = np.where(self.y_pred < threshold, 1, 0)\n",
    "        save_result(self.case_name, self.model_name, y_result, self.test_input_names, train=True, whole=True)\n",
    "\n",
    "    \n",
    "    def save_whole_train_region(self):\n",
    "        # Saves results for the entire training region\n",
    "        print(\"Train 전 지역에 대해 결과 저장 및 gpkg 생성 시작\")\n",
    "        threshold = self.result_dict[\"threshold\"]\n",
    "        reigion_input = input_data_process(image_to_array(self.input_names, self.size, self.img_channels), self.delete_channel)\n",
    "\n",
    "        y_pred = self.model.predict([reigion_input])\n",
    "        y_result = np.where(y_pred < threshold, 1, 0)\n",
    "        save_result(self.case_name, self.model_name, y_result, self.input_names, train=True, whole=True)\n",
    "\n",
    "    def save_whole_test_region(self, dict):\n",
    "        # Saves results for specific test regions\n",
    "        print(\"입력한 Test 지역에 대해 결과 저장 및 gpkg 생성 시작\")\n",
    "        threshold = self.result_dict[\"threshold\"]\n",
    "        test_region_input_names = make_nameslist(dict, image=True)\n",
    "        print(len(test_region_input_names))\n",
    "\n",
    "        flag_edge = split_samples_edge(test_region_input_names)\n",
    "        test_region_input_names = test_region_input_names[flag_edge]\n",
    "        print(len(test_region_input_names))\n",
    "\n",
    "        flag_rock = split_samples_rock(test_region_input_names)\n",
    "        test_region_input_names = test_region_input_names[flag_rock]\n",
    "        print(len(test_region_input_names))\n",
    "\n",
    "        flag_cloud = split_samples_cloud(test_region_input_names)\n",
    "        test_region_input_names = test_region_input_names[flag_cloud]\n",
    "        print(len(test_region_input_names))\n",
    "\n",
    "        print(f\"Let's make polygons with {len(test_region_input_names)} images from test regions\")\n",
    "\n",
    "        reigion_input = input_data_process(image_to_array(test_region_input_names, self.size, self.img_channels), self.delete_channel)\n",
    "\n",
    "        y_pred = self.model.predict([reigion_input])\n",
    "        y_result = np.where(y_pred < threshold, 1, 0)\n",
    "        save_result(self.case_name, self.model_name, y_result, test_region_input_names, train=False, whole=True)\n",
    "\n",
    "    def save_result_json(self):\n",
    "        # Saves the evaluation metrics and hyperparameters as a JSON file\n",
    "        json_save_path = os.path.join(self.base_path, self.case_name, self.model_name, self.model_name) + \".json\"\n",
    "        \n",
    "        # Combine result_dict and params into a single dictionary\n",
    "        data = {\n",
    "            \"result\": self.result_dict,\n",
    "            \"params\": self.params\n",
    "        }\n",
    "\n",
    "        with open(json_save_path, \"w\") as output_file:\n",
    "            json.dump(data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Band Information\n",
    "1. \"B\" : [0],\n",
    "2. \"G\" : [1],\n",
    "3. \"R\" : [2],\n",
    "4. \"NIR\" : [3],\n",
    "5. \"SWIR\" : [4],\n",
    "6. \"NDWI\" : [5],\n",
    "7. \"NDWI_ice\" : [6],\n",
    "8. \"NDSI\" : [7],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = \"/home/u2018144071/SIG/params2.json\"\n",
    "\n",
    "img_channels = 8\n",
    "size = 256\n",
    "n_case = 2\n",
    "\n",
    "channel_dict = {\n",
    "    '''\n",
    "    e.g If you want to combination of B,G & R\n",
    "    Input like this\n",
    "    \" B + G + R \" : [0,1,2]\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #B + G +  R\": [0,1,2],\n",
    "\n",
    "    \"NIR + NDWI + NDWI_ice + R + SWIR\" : [2,3,4,5,6],\n",
    "}\n",
    "\n",
    "total = 1\n",
    "rate = 0.03\n",
    "\n",
    "# dict : key : 날짜, items : region list \n",
    "#train_dict = {\"0603\" : [2,4,6], \"0619\" : [1,3,5], \"0731\" : [2,4,6], \"0825\" : [1,3,5]}    ###########\n",
    "#train_dict = {\"0603\" : [2,4,6]} # train에 사용할 data를 dict 형태로 넣어주세요.\n",
    "test_dict = {\"0603\" : [1,3,5]}  # 최종적으로 polygon을 생성할 data를 dict 형태로 넣어주세요.\n",
    "\n",
    "for case in range(n_case):   # n_case개의 case를 진행\n",
    "    \n",
    "    case_name = \"last_model_Case {0}_\".format(case+1)\n",
    "    #case_name = \"0603_Case {0}_param_add\".format(case+1)\n",
    "\n",
    "    #input_data_path = data_subsetting(train_dict, case_name, rate)          \n",
    "    input_data_path = \"/home/u2018144071/SIG/result/decid_model_Case 2_/decid_model_Case 2_.json\"   #####\n",
    "    print(input_data_path)\n",
    "\n",
    "    for model_name in channel_dict:\n",
    "        print(\"{0}번째 학습/case_name : {1}/model name : {2}\".format(total, case_name, model_name))\n",
    "        test = ourModel(case_name, model_name, input_data_path)\n",
    "\n",
    "        input_channels = channel_dict[model_name]\n",
    "        test.data_pre_processing(size, img_channels, input_channels)\n",
    "\n",
    "        test.model_training(params_path)\n",
    "        test.save_model()\n",
    "        test.save_result()\n",
    "        test.save_result_json()\n",
    "        \n",
    "        if test.result_dict[\"f1_score\"] > 0:\n",
    "            test.save_only_test_set()\n",
    "            test.save_whole_train_region()\n",
    "            test.save_whole_test_region(test_dict)   #######                 \n",
    "\n",
    "        total += 1\n",
    "        print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
